# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (n in seq(5, N, 5)){
# generate bootstrap indices for each datasets:
boot.indx <- boot(nrow(train_df), n, L)
train.index <- boot.indx[,(1:n)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
### for every dataset sizes:
for (l in 1:L){
# #### calculate iteration index i
# i <- (n-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[l,]
#### save the value of k and l
miss[i,'N'] <- n
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train_df[indx,-5 ], train_df[indx,5], test.data, K=K)  - test.label.1)**2)
i = i+1
}
}
miss
# plot misclassification percentage for train and test data sets
miss.m <- melt(miss, id=c('N', 'L')) # reshape for visualization
names(miss.m) <- c('size', 'L', 'type', 'miss')
ggplot(data=miss.m, aes(x=size, miss, color=type)) + geom_jitter(alpha=0.5)  +
scale_fill_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Jitter Plot)') + theme_minimal()
ggplot(data=miss.m[miss.m$type=='test',], aes(factor(size), miss,fill=type)) + geom_boxplot(outlier.shape = NA)  +
scale_color_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Error vs. Size (Box Plot)') + theme_minimal()
# ignore the warnings (because of ignoring outliers)
options(warn=-1)
K <- 20           # Maximum K for KNN
L <- 30           # number of datasets
N <- 60          # size of datasets
# generate bootstrap indices:
boot.indx <- boot(nrow(train_df), N, L)
train.index <- boot.indx[,(1:N)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))
#### save sample indices that were selected by bootstrap
indx <- boot.indx[l,]
boot.indx
K <- 20           # Maximum K for KNN
L <- 30           # number of datasets
N <- 60          # size of datasets
# generate bootstrap indices:
boot.indx <- boot(nrow(train_df), N, L)
train.index <- boot.indx[,(1:N)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (k in 1: K){
### for every dataset sizes:
for (l in 1:L){
#### calculate iteration index i
i <- (k-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[1,]
#### save the value of k and l
miss[i,'K'] <- k
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train.data[indx, ], train.label[indx], test.data, K=k)  - test.label.1)**2)
}
}
boot.indx[1,]
#### save sample indices that were selected by bootstrap
indx <- boot.indx[1,]
indx
train.data[indx, ]
train_df[indx, ]
indx
train.data
train.data[indx, ]
train_df[indx, ]
K <- 20           # Maximum K for KNN
L <- 30           # number of datasets
N <- 60          # size of datasets
# generate bootstrap indices:
boot.indx <- boot(nrow(train_df), N, L)
train.index <- boot.indx[,(1:N)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (k in 1: K){
### for every dataset sizes:
for (l in 1:L){
#### calculate iteration index i
i <- (k-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[1,]
#### save the value of k and l
miss[i,'K'] <- k
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train_df[indx, -5], train_df[indx,5], test.data, K=k)  - test.label.1)**2)
}
}
miss
K <- 20           # Maximum K for KNN
L <- 30           # number of datasets
N <- 60          # size of datasets
# generate bootstrap indices:
boot.indx <- boot(nrow(train_df), N, L)
train.index <- boot.indx[,(1:N)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (k in 1: K){
### for every dataset sizes:
for (l in 1:L){
#### calculate iteration index i
i <- (k-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[l,]
#### save the value of k and l
miss[i,'K'] <- k
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train_df[indx, -5], train_df[indx,5], test.data, K=k)  - test.label.1)**2)
}
}
miss
# plot misclassification percentage for train and test data sets
miss.m <- melt(miss, id=c('K', 'L')) # reshape for visualization
names(miss.m) <- c('K', 'L', 'type', 'miss')
ggplot(data=miss.m, aes(x=K, miss, color=type)) + geom_jitter(alpha=0.5)  +
scale_fill_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Jitter Plot)') + theme_minimal()
ggplot(data=miss.m[miss.m$type=='test',], aes(factor(K), miss,fill=type)) + geom_boxplot(outlier.shape = NA)  +
scale_color_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Box Plot)') + theme_minimal()
# ignore the warnings (because of ignoring outliers)
options(warn=-1)
K <- 20           # Maximum K for KNN
L <- 30           # number of datasets
N <- 60          # size of datasets
# generate bootstrap indices:
boot.indx <- boot(nrow(train_df), N, L)
train.index <- boot.indx[,(1:N)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (k in 1: K){
### for every dataset sizes:
for (l in 1:L){
#### calculate iteration index i
i <- (k-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[1,]
#### save the value of k and l
miss[i,'K'] <- k
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train.data[indx, ], train.label[indx], test.data, K=k)  - test.label.1)**2)
}
}
miss
K <- 20           # Maximum K for KNN
L <- 30           # number of datasets
N <- 60          # size of datasets
# generate bootstrap indices:
boot.indx <- boot(nrow(train_df), N, L)
train.index <- boot.indx[,(1:N)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('K'=1:K, 'L'=1:L, 'test'=rep(0,L*K))
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (k in 1: K){
### for every dataset sizes:
for (l in 1:L){
#### calculate iteration index i
i <- (k-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[l,]
#### save the value of k and l
miss[i,'K'] <- k
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train.data[indx, ], train.label[indx], test.data, K=k)  - test.label.1)**2)
}
}
miss
miss
# plot misclassification percentage for train and test data sets
miss.m <- melt(miss, id=c('K', 'L')) # reshape for visualization
names(miss.m) <- c('K', 'L', 'type', 'miss')
ggplot(data=miss.m, aes(x=K, miss, color=type)) + geom_jitter(alpha=0.5)  +
scale_fill_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Jitter Plot)') + theme_minimal()
ggplot(data=miss.m[miss.m$type=='test',], aes(factor(K), miss,fill=type)) + geom_boxplot(outlier.shape = NA)  +
scale_color_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Box Plot)') + theme_minimal()
# ignore the warnings (because of ignoring outliers)
options(warn=-1)
K <- 5          # Maximum K for KNN
L <- 50          # number of datasets
N <- 75          # size of datasets
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('N'=seq(5,N,5), 'L'=1:L, 'test'=rep(0,L*N/5))
i <- 1
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (n in seq(5, N, 5)){
# generate bootstrap indices for each datasets:
boot.indx <- boot(nrow(train_df), n, L)
train.index <- boot.indx[,(1:n)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
### for every dataset sizes:
for (l in 1:L){
# #### calculate iteration index i
# i <- (n-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[l,]
#### save the value of k and l
miss[i,'N'] <- n
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train.data[indx, ], train.label[indx], test.data, K=K)  - test.label.1)**2)
i = i+1
}
}
miss
# plot misclassification percentage for train and test data sets
miss.m <- melt(miss, id=c('N', 'L')) # reshape for visualization
names(miss.m) <- c('size', 'L', 'type', 'miss')
ggplot(data=miss.m, aes(x=size, miss, color=type)) + geom_jitter(alpha=0.5)  +
scale_fill_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Jitter Plot)') + theme_minimal()
ggplot(data=miss.m[miss.m$type=='test',], aes(factor(size), miss,fill=type)) + geom_boxplot(outlier.shape = NA)  +
scale_color_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Error vs. Size (Box Plot)') + theme_minimal()
# ignore the warnings (because of ignoring outliers)
options(warn=-1)
K <- 5          # Maximum K for KNN
L <- 50          # number of datasets
N <- 75          # size of datasets
# a dataframe to track the number of missclassified samples in each case
miss <- data.frame('N'=seq(5,N,5), 'L'=1:L, 'test'=rep(0,L*N/5))
i <- 1
# THIS MAY TAKE A FEW MINUTES TO COMPLETE
## for every k values:
for (n in seq(5, N, 5)){
# generate bootstrap indices for each datasets:
boot.indx <- boot(nrow(train_df), n, L)
train.index <- boot.indx[,(1:n)]
train.data <- train_df[train.index, -5]
train.label <- train_df[train.index, 5]
test.data <- train_df[-train.index, -5]
test.label.1 <- train_df[-train.index, 5]
### for every dataset sizes:
for (l in 1:L){
# #### calculate iteration index i
# i <- (n-1)*L+l
#### save sample indices that were selected by bootstrap
indx <- boot.indx[l,]
#### save the value of k and l
miss[i,'N'] <- n
miss[i,'L'] <- l
#### calculate and record the train and test missclassification rates
miss[i,'test'] <-  sum((knn(train.data[indx, ], train.label[indx], test.data, K=K)  - test.label.1)**2)
i = i+1
}
}
miss
# plot misclassification percentage for train and test data sets
miss.m <- melt(miss, id=c('N', 'L')) # reshape for visualization
names(miss.m) <- c('size', 'L', 'type', 'miss')
ggplot(data=miss.m, aes(x=size, miss, color=type)) + geom_jitter(alpha=0.5)  +
scale_fill_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Missclassifcation vs. K (Jitter Plot)') + theme_minimal()
ggplot(data=miss.m[miss.m$type=='test',], aes(factor(size), miss,fill=type)) + geom_boxplot(outlier.shape = NA)  +
scale_color_discrete(guide = guide_legend(title = NULL)) +
ggtitle('Error vs. Size (Box Plot)') + theme_minimal()
# ignore the warnings (because of ignoring outliers)
options(warn=-1)
knitr::opts_chunk$set(echo = TRUE)
library(reshape2)
library(ggplot2)
library(reshape2)
library(corrplot)
library(dplyr)
install.packages("dplyr")
library(dplyr)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
Sys.which("make")
install.packages('pillar)
install.packages('pillar')
install.packages('pillar')
library(ggplot2)
library(reshape2)
library(ggplot2)
library(corrplot)
library(dplyr)
install.packages('dplyr')
install.packages('dplyr')
install.packages("dplyr")
knitr::opts_chunk$set(echo = TRUE)
library(timetk)
install.packages('timetk'
)
library('timetk'
)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
train_df <- read.csv('Task2B_train.csv')
test_df <- read.csv('Task2B_test.csv')
train_df
test_df
train_df <- train_df[complete.cases(train_df),]
names(train_df)[3] <- 'label'
train_df
test_df <- test_df[complete.cases(test_df),]
names(test_df)[3] <- 'label'
test_df
## Take a look at the data set
ggplot(data=train_df, aes(x=x1, y=x2, color=label, label=ifelse(label==0, '0', '1'))) +
geom_text(size = 4, alpha=1) + ggtitle ('Data set') + theme_minimal()
# No. of data points to work with
N <- nrow(train_df)
# Splitting the data into train and validation set
train.len <- round(N*0.8)
train.index <- sample(1:N,train.len)
train.data <- train_df[train.index, 1:2]
train.label <- train_df[train.index, 3]
test.data <- train_df[-train.index, 1:2]
test.label <- train_df[-train.index, 3]
perceptron <- function(x, y, eta, niter) {
# initialize weight vector
W <- data.frame(matrix(,nrow=length(unique(train_df$y)), ncol=ncol(Phi))) # Empty Weight vector
W[1,] <- runif(ncol(Phi)) # Random initial values for weight vector
W[2,] <- runif(ncol(Phi)) # Random initial values for weight vector
#     W[3,] <- runif(ncol(Phi)) # Random initial values for weight vector
names(W) <- c("w0", "w1", "w2")
W <- as.matrix(W)
# Run until stopping condition is met
while(!terminate){
train.index <- sample(1:length(x), replace = FALSE)
Phi <- Phi[train.index,]
T <- T[train.index]
# Run through all the data points in training dataset
for (i in 1:length(train.index)){
if (tau == tau.max) {break}
if (which.max(W%*%Phi[i,]) == 0){
y <- 0
}
if (which.max(W%*%Phi[i,]) == 1){
y <- 1
}
# Check if predicted is same as the true label
if (y != T[i]){
tau <- tau + 1
#             y <- eval(parse(text=strsplit(y, "C")[[1]][2]))
#             t <- eval(parse(text=strsplit(T[i], "C")[[1]][2]))
W[y,] = W[y,] + eta*Phi[i,]
W[t,] = W[t,] - eta*Phi[i,]
}
}
# Terminating condition
terminate <- tau >= tau.max
}
W <- add_column(data.frame(W), model=c("(perceptron-1)", "(perceptron-2)", "(perceptron-3)"), .before = "w0")
return(W)
}
train.data
train_df <- read.csv('Task2B_train.csv')
test_df <- read.csv('Task2B_test.csv')
train_df
test_df
train_df <- train_df[complete.cases(train_df),]
names(train_df)[3] <- 'label'
train_df
test_df <- test_df[complete.cases(test_df),]
names(test_df)[3] <- 'label'
test_df
## Take a look at the data set
ggplot(data=train_df, aes(x=x1, y=x2, color=label, label=ifelse(label==0, '0', '1'))) +
geom_text(size = 4, alpha=1) + ggtitle ('Data set') + theme_minimal()
# No. of data points to work with
N <- nrow(train_df)
# Splitting the data into train and validation set
train.len <- round(N*0.8)
train.index <- sample(1:N,train.len)
train.data <- train_df[train.index, 1:2]
train.label <- train_df[train.index, 3]
test.data <- train_df[-train.index, 1:2]
test.label <- train_df[-train.index, 3]
perceptron <- function(x, y, eta, niter) {
# initialize weight vector
W <- data.frame(matrix(,nrow=length(unique(train_df$y)), ncol=ncol(Phi))) # Empty Weight vector
W[1,] <- runif(ncol(Phi)) # Random initial values for weight vector
W[2,] <- runif(ncol(Phi)) # Random initial values for weight vector
#     W[3,] <- runif(ncol(Phi)) # Random initial values for weight vector
names(W) <- c("w0", "w1", "w2")
W <- as.matrix(W)
# Run until stopping condition is met
while(!terminate){
train.index <- sample(1:length(x), replace = FALSE)
Phi <- Phi[train.index,]
T <- T[train.index]
# Run through all the data points in training dataset
for (i in 1:length(train.index)){
if (tau == tau.max) {break}
if (which.max(W%*%Phi[i,]) == 0){
y <- 0
}
if (which.max(W%*%Phi[i,]) == 1){
y <- 1
}
# Check if predicted is same as the true label
if (y != T[i]){
tau <- tau + 1
#             y <- eval(parse(text=strsplit(y, "C")[[1]][2]))
#             t <- eval(parse(text=strsplit(T[i], "C")[[1]][2]))
W[y,] = W[y,] + eta*Phi[i,]
W[t,] = W[t,] - eta*Phi[i,]
}
}
# Terminating condition
terminate <- tau >= tau.max
}
W <- add_column(data.frame(W), model=c("(perceptron-1)", "(perceptron-2)", "(perceptron-3)"), .before = "w0")
return(W)
}
## Basis function (Step 1)
Phi <- as.matrix(cbind(1, train.data)) # add a column of 1 as phi_0
# Initialization
eta <- 0.01 # Learning rate
epsilon <- 0.001 # Stoping criterion
tau.max <- 1000 # Maximum number of iterations
train.data
Phi
## Basis function (Step 1)
Phi <- as.matrix(cbind(1, train.data)) # add a column of 1 as phi_0
# Initialization
eta <- 0.01 # Learning rate
epsilon <- 0.001 # Stoping criterion
tau.max <- 1000 # Maximum number of iterations
# Taking the response values in a variable
T <- train.label
tau <- 1 # iteration counter
terminate <- FALSE # termination status
perceptron(train.data, T, 0.01, 100)
rsconnect::setAccountInfo(name='overnight-hospital',
token='98098664743EA96555C19207A8DB8269',
secret='ecqEeizWul82BuVT0bvMy+8172SkYCjrudC7qzA3')
shiny::runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
rsconnect::setAccountInfo(name='overnight-hospital',
token='98098664743EA96555C19207A8DB8269',
secret='ecqEeizWul82BuVT0bvMy+8172SkYCjrudC7qzA3')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
/var/log/shiny-server.log
/var/log/shiny-server.log
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
options(shiny.reactlog=TRUE)
runApp('C:/Users/Sai Ram/OneDrive/Semester-4/FIT5120/Rshiny/Dashboard/1.5')
